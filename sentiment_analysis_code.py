# -*- coding: utf-8 -*-
"""Sentiment_Analysis_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JpY4mJr4urLbjLibFVsx_As7R0Z3pJdE
"""

!pip install requests beautifulsoup4 pandas openpyxl nltk textblob

import pandas as pd

# Load the Excel file (already uploaded to /mnt/data)
input_path = "/content/Input.xlsx"
df = pd.read_excel(input_path)

# Show first few entries
df.head()

import os
import requests
from bs4 import BeautifulSoup

# Create folder to store articles
os.makedirs("articles", exist_ok=True)

def extract_article_text(url):
    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')

        title = soup.find('h1')
        content = soup.find('div', class_='td-post-content')  # Blackcoffer articles use this class

        title_text = title.get_text(strip=True) if title else ''
        content_text = content.get_text(separator=' ', strip=True) if content else ''

        return title_text + "\n\n" + content_text
    except Exception as e:
        print(f"❌ Error fetching {url}: {e}")
        return ''

# Loop through each URL and save .txt file
for _, row in df.iterrows():
    url_id = row['URL_ID']
    url = row['URL']
    article = extract_article_text(url)

    if article.strip():  # Save only if not empty
        with open(f"articles/{url_id}.txt", "w", encoding="utf-8") as f:
            f.write(article)
    else:
        print(f"⚠️ Empty article for URL_ID {url_id}")

import re
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from textblob import TextBlob

nltk.download('punkt')

# Define helper functions
def count_syllables(word):
    word = word.lower()
    vowels = "aeiou"
    count = 0
    if word[0] in vowels:
        count += 1
    for i in range(1, len(word)):
        if word[i] in vowels and word[i - 1] not in vowels:
            count += 1
    if word.endswith("e"):
        count -= 1
    if count == 0:
        count += 1
    return count

def is_complex(word):
    return count_syllables(word) >= 3

def count_pronouns(text):
    return len(re.findall(r'\b(I|we|my|ours|us)\b', text, flags=re.I))

# Example positive/negative word lists (use full dictionaries if provided)
positive_words = set(["good", "great", "positive", "fortunate", "excellent", "correct", "superior", "benefit"])
negative_words = set(["bad", "worst", "negative", "unfortunate", "wrong", "inferior", "harm"])

!pip install chardet

import chardet

def detect_encoding(filepath):
    with open(filepath, 'rb') as f:
        raw_data = f.read()
    result = chardet.detect(raw_data)
    return result['encoding']

def load_words(filepath):
    encoding = detect_encoding(filepath)
    with open(filepath, 'r', encoding=encoding) as f:
        return set(line.strip() for line in f if line.strip() and not line.startswith(';'))

# Load files
positive_words = load_words('/content/positive-words.txt')
negative_words = load_words('/content/negative-words.txt')

import pandas as pd

# Load Excel file
df = pd.read_excel('/content/Input.xlsx')  # or the path where you've uploaded it

# Show column names to verify
print("Columns in Excel:", df.columns.tolist())

def analyze_sentiment(text, pos_words, neg_words):
    words = text.lower().split()
    pos_count = sum(1 for word in words if word in pos_words)
    neg_count = sum(1 for word in words if word in neg_words)

    if pos_count > neg_count:
        return 'Positive'
    elif neg_count > pos_count:
        return 'Negative'
    else:
        return 'Neutral'

TEXT_COLUMN = 'Content'

TEXT_COLUMN = 'YourCorrectColumnName'  # e.g., 'Review', 'Text', etc.

print(df.columns.tolist())

!pip install --upgrade --force-reinstall newspaper3k

!pip install lxml[html_clean]

import newspaper
print("Newspaper version:", newspaper.__version__)

from newspaper import Article
import pandas as pd

# Load input Excel file
df = pd.read_excel('/content/Input.xlsx')  # replace with your actual file path

# Function to extract article text
def extract_article(url):
    try:
        article = Article(url)
        article.download()
        article.parse()
        return article.text
    except:
        return ""

# Create 'Content' column by applying extraction
df['Content'] = df['URL'].apply(extract_article)

# Save to new file
df.to_excel('output_with_content.xlsx', index=False)

# Load word lists
def load_words(filepath):
    with open(filepath, 'r', encoding='ISO-8859-1') as file:
        words = [line.strip() for line in file if line.strip() and not line.startswith(';')]
    return set(words)

positive_words = load_words('/content/positive-words.txt')
negative_words = load_words('/content/negative-words.txt')

# Function to analyze sentiment
def analyze_sentiment(text, pos_words, neg_words):
    text = text.lower()
    words = text.split()
    pos_count = sum(1 for word in words if word in pos_words)
    neg_count = sum(1 for word in words if word in neg_words)

    if pos_count > neg_count:
        return "Positive"
    elif neg_count > pos_count:
        return "Negative"
    else:
        return "Neutral"

# Load the file with content
df = pd.read_excel('output_with_content.xlsx')

# Apply sentiment analysis
df['Sentiment'] = df['Content'].apply(lambda x: analyze_sentiment(str(x), positive_words, negative_words))

# Save final output
df.to_excel('final_output_with_sentiment.xlsx', index=False)

from google.colab import files
files.download('final_output_with_sentiment.xlsx')

!pip install newspaper3k

!pip install newspaper3k
!pip install lxml_html_clean

from newspaper import Article

import pandas as pd
from newspaper import Article

df = pd.read_excel('/content/Input.xlsx')

def extract_article(url):
    try:
        article = Article(url)
        article.download()
        article.parse()
        return article.text
    except:
        return ""

df['Content'] = df['URL'].apply(extract_article)
df.to_excel('output_with_content.xlsx', index=False)

from google.colab import files
files.download("output_with_content.xlsx")